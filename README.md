# ğŸ§  CoDA: Context-Decoupled Hierarchical Agent

**CoDA-Gemma2-RED** â€” A single Gemma-2-2B model trained as a hierarchical RAG agent using GRPO reinforcement learning.

[![Model on HF](https://img.shields.io/badge/ğŸ¤—-Model-yellow)](https://huggingface.co/Phonsiri/CoDA-Gemma2-RED-v1)
[![W&B Dashboard](https://img.shields.io/badge/W%26B-Dashboard-blue)](https://wandb.ai)

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Gemma-2-2B (Single LLM)         â”‚
â”‚                                              â”‚
â”‚   ğŸ§  Planner          âš¡ Executor            â”‚
â”‚   (Strategic)         (Ephemeral)            â”‚
â”‚   Plans long-term     Executes subtasks      â”‚
â”‚   Keeps context       Forgets after done     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                       â”‚
       â–¼                       â–¼
  search(query)          finish(answer)
       â”‚
       â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ FAISS Index â”‚ â† Wikipedia (21M docs)
  â”‚ (CPU)       â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Concepts

| Concept | Description |
|---------|-------------|
| **Context-Decoupled** | Separates Planner (strategic) from Executor (ephemeral) contexts to prevent context explosion |
| **PECO Training** | Planner-Executor Co-Optimization â€” trains both roles simultaneously with RL |
| **GRPO** | Group Relative Policy Optimization for reward-based learning |
| **RED** | Recall-Extend Dynamics for balancing SFT/RL training |

### Composite Reward (3 components)

1. **Correctness** â€” F1 score vs ground truth answer (primary)
2. **Format Compliance** â€” Correct XML tag usage (+0.1)
3. **Refinement Quality** â€” Effective search summarization (+0.1)

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.12+
- CUDA 12.x compatible GPU (H100 recommended)
- ~140GB disk space (for retriever index + Wikipedia corpus)

### 1. Clone & Install

```bash
git clone https://github.com/Phonsiriwillbejommarn/CoDA.git
cd CoDA
pip install -e .
```

### 2. Login Services

```bash
wandb login          # For training dashboard
huggingface-cli login  # For checkpoint push
```

### 3. Download Data

```bash
# Download retriever index + Wikipedia corpus (~130GB)
bash preprocess/download_and_process.sh

# Process training data (NQ, HotpotQA, TriviaQA, PopQA, Musique, etc.)
bash preprocess/scripts/data_process.sh

# Generate SFT training data
python cmd/generate_sft_data.py
```

### 4. Start Training

```bash
# Terminal 1: Start Retrieval Server
bash retrieval_launch.sh

# Terminal 2: Start Training
bash cmd/train.sh
```

---

## âš™ï¸ Training Configuration

All configs are in [`cmd/train.sh`](cmd/train.sh):

| Parameter | Default | Description |
|-----------|---------|-------------|
| `train_batch_size` | 32 | Prompts per training step |
| `n_agent` | 2 | Responses per prompt (GRPO group size) |
| `max_turns` | 2 | Search rounds per sample |
| `save_freq` | 5 | Checkpoint push frequency (steps) |
| `total_training_steps` | 480 | Total training steps |
| `max_prompt_length` | 3072 | Max prompt token length |
| `max_response_length` | 1024 | Max response token length |
| `learning_rate` | 1e-6 | Actor learning rate |

### Speed Tuning
- **Faster:** Reduce `max_turns`, `n_agent`, `train_batch_size`
- **Better learning:** Increase `max_turns` (slower per step)

### Checkpoint Management
- Saves every `save_freq` steps to local + [HF Hub](https://huggingface.co/Phonsiri/CoDA-Gemma2-RED-v1)
- **Keeps only 2 latest checkpoints** (auto-deletes old ones)
- Auto-resumes from the latest checkpoint on restart

---

## ğŸ“ Project Structure

```
CoDA/
â”œâ”€â”€ cmd/
â”‚   â”œâ”€â”€ train.sh                 # Main training script & config
â”‚   â”œâ”€â”€ auto_resume.py           # Auto-resume from HF Hub checkpoints
â”‚   â””â”€â”€ generate_sft_data.py     # Generate SFT training data
â”œâ”€â”€ preprocess/
â”‚   â”œâ”€â”€ download_and_process.sh  # Download retriever data
â”‚   â””â”€â”€ scripts/
â”‚       â””â”€â”€ data_process.sh      # Process QA datasets
â”œâ”€â”€ search_r1/
â”‚   â”œâ”€â”€ llm_agent/
â”‚   â”‚   â””â”€â”€ generation.py        # Agent generation logic (Planner/Executor)
â”‚   â””â”€â”€ search/
â”‚       â””â”€â”€ retrieval_server.py  # FastAPI retrieval server (FAISS)
â”œâ”€â”€ verl/
â”‚   â”œâ”€â”€ trainer/
â”‚   â”‚   â”œâ”€â”€ main_ppo.py          # Training entry point
â”‚   â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”‚   â””â”€â”€ grpo_trainer.yaml # Default config
â”‚   â”‚   â””â”€â”€ ppo/
â”‚   â”‚       â”œâ”€â”€ ray_trainer.py   # Main training loop + checkpointing
â”‚   â”‚       â””â”€â”€ core_algos.py    # GRPO algorithm implementation
â”‚   â”œâ”€â”€ workers/
â”‚   â”‚   â”œâ”€â”€ actor/
â”‚   â”‚   â”‚   â””â”€â”€ dp_actor.py      # Actor policy update
â”‚   â”‚   â”œâ”€â”€ fsdp_workers.py      # FSDP distributed workers
â”‚   â”‚   â””â”€â”€ rollout/
â”‚   â”‚       â””â”€â”€ vllm_rollout/    # vLLM inference engine
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ reward_score/
â”‚       â”‚   â””â”€â”€ qa_em.py         # Reward functions (F1, EM)
â”‚       â”œâ”€â”€ dataset/
â”‚       â”‚   â”œâ”€â”€ rl_dataset.py    # RL training dataset
â”‚       â”‚   â””â”€â”€ sft_dataset.py   # SFT co-training dataset
â”‚       â””â”€â”€ padding_utils.py     # SDPA padding utilities
â”œâ”€â”€ data/                        # Training data (generated, not in git)
â”œâ”€â”€ retrieval_launch.sh          # Launch retrieval server
â””â”€â”€ requirements.txt             # Python dependencies
```

---

## ğŸ“Š Performance

| Metric | Value |
|--------|-------|
| Step Time | ~4-5 min (H100 1x) |
| Total Steps | 480 |
| Estimated Duration | ~35 hours |
| Samples per Step | 64 (32 prompts Ã— 2 responses) |
| Model Size | 2B parameters |

---

## ğŸ”§ Restart After Server Reboot

Data files are ephemeral on cloud servers. After restart:

```bash
cd CoDA
git pull origin main
bash preprocess/scripts/data_process.sh    # Recreate parquet files
python cmd/generate_sft_data.py            # Recreate SFT data
bash retrieval_launch.sh &                 # Start retriever
bash cmd/train.sh                          # Auto-resumes from HF Hub
```

> **Note:** If `wiki-18.jsonl` and `e5_Flat.index` are also missing, run `bash preprocess/download_and_process.sh` first.

---

## ğŸ“ License

Apache License 2.0

## ğŸ™ Acknowledgments

- Based on [Search-R1](https://github.com/PeterGriffinJin/Search-R1) framework
- Uses [verl](https://github.com/volcengine/verl) for RL training
- Model: [Google Gemma-2-2B](https://huggingface.co/google/gemma-2-2b)