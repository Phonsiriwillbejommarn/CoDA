# CoDA 2.0: RED Edition (Gemma-2-2b Optimized for 1x H100)
data:
  tokenizer: null
  train_files: ~/data/rlhf/gsm8k/train.parquet
  val_files: ~/data/rlhf/gsm8k/test.parquet
  prompt_key: prompt
  max_prompt_length: 4096   # เพิ่มความยาวเพื่อให้รองรับการทำงานแบบ Hierarchical ของ CoDA
  max_response_length: 1024  # Gemma 2 ทำงานได้ดีกับคำตอบที่มีรายละเอียด
  max_start_length: 2048
  max_obs_length: 512
  train_batch_size: 128      # ปรับให้เหมาะสมกับการสำรวจ (Exploration) บน 1 GPU ตามหลัก RED
  val_batch_size: 128
  shuffle_train_dataloader: True
  model_type: base

actor_rollout_ref:
  hybrid_engine: True
  model:
    path: google/gemma-2-2b   # โมเดล Gemma 2 สำหรับ SLM Reasoning
    enable_gradient_checkpointing: True # สำคัญมากสำหรับรันบน 1 GPU เพื่อประหยัด Memory
    use_remove_padding: True
  actor:
    strategy: fsdp
    ppo_mini_batch_size: 32   # ลดขนาดลงเพื่อให้เหมาะสมกับ 1 GPU
    ppo_micro_batch_size: 8   # ลดขนาดลงเพื่อให้เหมาะสมกับ 1 GPU
    grad_clip: 1.0
    state_masking: True       # เปิดใช้งานเพื่อแยกบริบท Planner และ Executor ของ CoDA
    clip_ratio: 0.2
    entropy_coeff: 0.001
    use_kl_loss: True         # ต้องเป็น True สำหรับอัลกอริทึม GRPO
    kl_loss_coef: 0.001
    kl_loss_type: low_var_kl
    ppo_epochs: 1
    shuffle: False
    refine_score: 0.1         # ให้คะแนนการกลั่นกรองข้อมูลตามแนวทาง CoDA
    format_score: 0.1         # ให้คะแนนรูปแบบ XML เพื่อความเป็นระเบียบ
    optim:
      lr: 1e-6
      lr_warmup_steps_ratio: 0.1
      warmup_style: constant
      total_training_steps: 480 # จุดที่ประสิทธิภาพสูงสุดตามกราฟของ RED
  rollout:
    name: vllm
    temperature: 0.9          # ปรับลดเล็กน้อยเพื่อความแม่นยำของ Gemma 2
    top_k: -1
    top_p: 0.95
    response_length: ${data.max_response_length}
    dtype: bfloat16
    gpu_memory_utilization: 0.75 # จองพื้นที่ให้ vLLM มากขึ้นสำหรับ H100 80GB
    enforce_eager: True
    free_cache_engine: True
    tensor_model_parallel_size: 1 # ต้องเป็น 1 สำหรับการใช้งาน 1 GPU
    max_num_batched_tokens: 8192
    n: 1
    n_agent: 8                # จำนวนเอเจนท์ขนานสำหรับงานวางแผนของ CoDA

critic:
  strategy: fsdp
  optim:
    lr: 1e-5
    total_training_steps: 480
  model:
    path: ${actor_rollout_ref.model.path}
    enable_gradient_checkpointing: True
  ppo_mini_batch_size: 32
  ppo_micro_batch_size: 8

algorithm:
  gamma: 1.0
  lam: 1.0
  adv_estimator: grpo        # ใช้การประมาณค่าแบบ GRPO ตามที่ RED แนะนำ
  no_think_rl: False
  # --- RED Logic Implementation ---
  red_enabled: True                 # เปิดใช้งาน Recall-Extend Dynamics
  entropy_weight_regulation: True   # คุมสมดุลการใช้ SFT และ RL ด้วย Entropy
  accuracy_aware_policy_shift: True # ปรับเปลี่ยนนโยบายตามความแม่นยำของคำตอบ
  # --------------------------------
  kl_ctrl:
    type: fixed
    kl_coef: 0.001
  state_masking:
    start_state_marker: "<documents>"
    end_state_marker: "</documents>"

trainer:
  total_training_steps: 480
  project_name: CoDA_RED_Gemma2
  experiment_name: gemma2_2b_h100_v1
  logger: [ 'console', 'tensorboard' ]
  nnodes: 1
  n_gpus_per_node: 1                # ตั้งค่าสำหรับ H100 1 ใบ
  save_freq: 20
  test_freq: 9999
  default_local_dir: checkpoints/${trainer.project_name}/${trainer.experiment_name}

max_turns: 6      # คุมจำนวนรอบเพื่อแก้ปัญหา Context Explosion ของ CoDA
do_search: true
retriever:
  url: "http://127.0.0.1:8000/retrieve"
  topk: 3